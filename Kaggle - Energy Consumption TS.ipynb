{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Import data\n",
    "2. EDA\n",
    "3. Define functions\n",
    "4. Data cleaning\n",
    "5. Feature engineering\n",
    "6. Modeling\n",
    "7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r\"C:\\Users\\mcdonnelldeclan\\Downloads\\test.csv\")\n",
    "train = pd.read_csv(r\"C:\\Users\\mcdonnelldeclan\\Downloads\\train.csv\")\n",
    "test_features = pd.read_csv(r\"C:\\Users\\mcdonnelldeclan\\Downloads\\test_features.csv\")\n",
    "train_features = pd.read_csv(r\"C:\\Users\\mcdonnelldeclan\\Downloads\\train_features.csv\")\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what dates is the dataset for?\n",
    "    # All of 2016\n",
    "    # train.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the time interval?\n",
    "    # hourly, with no missing data or outliers (8784 hourly readings = 366 days in the year))\n",
    "    # timestamps = train['timestamp'].drop_duplicates().diff()\n",
    "    # timestamps.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many buildings in the dataset?\n",
    "    # 200 buildings, around half with all readings, and a minimum of ~311 days (7471 hourly readings)\n",
    "    # train['building_id'].value_counts(sort=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many anomalous datapoints?\n",
    "    # 37296, or 2.1%\n",
    "    # train.value_counts(subset='anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each building, create an IQR and label each point as an anomaly or not\n",
    "    # define a function that takes a building, creates an IQR, and labels each datapoint as anomalous or not\n",
    "    # for each bldg, calc IQR and upper/lower bounds\n",
    "\n",
    "def plot_anomalies(df, building_id, pred_anomaly):\n",
    "    \"\"\"\n",
    "    Take any building you want and plot what it looks like with the predicted anomalies labeled\n",
    "    \"\"\"\n",
    "    df[df['building_id'] == building_id].plot(kind='scatter', x='timestamp', y='meter_reading', figsize=(12,6)\n",
    "                                              , c=pred_anomaly, colormap='Paired'\n",
    "                                              , title=f'meter readings for building {building_id}')\n",
    "\n",
    "def eval_metrics(df, pred_anomaly, building_id=None):\n",
    "    \"\"\"\n",
    "    Take a dataframe and calculate the accuracy, precision, recall, F1 score, and AUC\n",
    "    \"\"\"\n",
    "    eval_df = df[df['building_id']==building_id].copy() if building_id is not None else df.copy()\n",
    "\n",
    "    accuracy = accuracy_score(eval_df['anomaly'], eval_df[pred_anomaly])\n",
    "    precision = precision_score(eval_df['anomaly'], eval_df[pred_anomaly])\n",
    "    recall = recall_score(eval_df['anomaly'], eval_df[pred_anomaly])\n",
    "    f1 = f1_score(eval_df['anomaly'], eval_df[pred_anomaly])\n",
    "    \n",
    "    if building_id is not None:\n",
    "        print('Building ID: ', building_id)\n",
    "    print('Accuracy: ', round(accuracy * 100, 2), '%')\n",
    "    print('Recall: ', round(recall * 100, 2), '%')\n",
    "    print('Precision: ', round(precision * 100, 2), '%')\n",
    "    print('F1 Score: ', round(f1 * 100, 2), '%')\n",
    "\n",
    "def label_anomalies_iqr(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a dataset of buildings and meter readings. For each building, calculates Q3/Q1/IQR and upper/lower limits \n",
    "    of where outliers are. It then uses those limits to label a new column called pred_anomaly_iqr as 1/0.\n",
    "    \"\"\"\n",
    "    # do the calcs\n",
    "    readings = df['meter_reading']\n",
    "    q3 = readings.quantile(0.75)\n",
    "    q1 = readings.quantile(0.25)\n",
    "\n",
    "    IQR = q3 - q1\n",
    "    factor = 1.5\n",
    "\n",
    "    upper_limit = q3 + (IQR * factor)\n",
    "    lower_limit = q1 - (IQR * factor)\n",
    "\n",
    "    # create new column label\n",
    "    df['pred_anomaly_iqr'] = np.where((df['meter_reading'] > upper_limit)\n",
    "                                                | (df['meter_reading'] < lower_limit)\n",
    "                                                , 1, 0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def anomalies_iqr(df):\n",
    "    \"\"\"\n",
    "    Create a pred_anomaly_iqr label using the IQR to determine outliers\n",
    "    \"\"\"\n",
    "    df_with_anomalies = df.groupby('building_id').apply(label_anomalies_iqr, include_groups=False)\n",
    "    df_with_anomalies.reset_index(inplace=True, level=0)\n",
    "    return df_with_anomalies\n",
    "\n",
    "def iforest_model(df):\n",
    "    \"\"\"\n",
    "    Takes the train dataframe and builds an iforest model for each building\n",
    "    \"\"\"\n",
    "    # create features for month/day/hour\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # instantiate an iforest model and scaler for the features\n",
    "    iforest_df = pd.DataFrame()\n",
    "    scaler = StandardScaler()\n",
    "    iso_forest = IsolationForest(n_estimators=150, contamination=0.025, random_state=1)\n",
    "    features_to_scale = ['hour', 'day', 'month', 'meter_reading_imputed']\n",
    "\n",
    "    # loop through each building\n",
    "    for bldg in df['building_id'].unique():\n",
    "        bldg_df = df[df['building_id']==bldg].copy()\n",
    "        # impute missing meter readings with mean values\n",
    "        bldg_df['meter_reading_imputed'] = bldg_df['meter_reading'].fillna(bldg_df['meter_reading'].mean())\n",
    "        # scale the features using standardization\n",
    "        X = scaler.fit_transform(bldg_df[features_to_scale])\n",
    "        # predict anomaly scores / anomalies with iso forest\n",
    "        iso_forest.fit(X)\n",
    "        bldg_df['pred_anomaly_iforest'] = (iso_forest.predict(X) == -1).astype(int) # iso forest labels anomalies as -1\n",
    "        bldg_df['anomaly_score'] = -iso_forest.score_samples(X) # in iso forest more negative scores = more likely to be an anomaly \n",
    "        # concat dataframes together into final df\n",
    "        iforest_df = pd.concat([iforest_df, bldg_df])\n",
    "    return iforest_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "- Detect anomalies with IQR and evaluate with accuracy/precision/recall/F1 score\n",
    "- Detect anomalies with Isolation Forest and evaluate with AUC and confusion matrix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  96.45 %\n",
      "Recall:  29.15 %\n",
      "Precision:  23.31 %\n",
      "F1 Score:  25.91 %\n"
     ]
    }
   ],
   "source": [
    "# detect anomalies with IQR\n",
    "train_with_anomalies = anomalies_iqr(train)\n",
    "# evaluate\n",
    "eval_metrics(train_with_anomalies, pred_anomaly='pred_anomaly_iqr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7351596103278517\n",
      "Building ID:  108\n",
      "Accuracy:  97.63 %\n",
      "Recall:  59.8 %\n",
      "Precision:  67.42 %\n",
      "F1 Score:  63.38 %\n"
     ]
    }
   ],
   "source": [
    "# detect anomalies with Isolation Forest\n",
    "iforest_df = iforest_model(train)\n",
    "\n",
    "# evaluate\n",
    "auc = roc_auc_score(iforest_df['anomaly'], iforest_df['anomaly_score'])\n",
    "print(auc)\n",
    "eval_metrics(iforest_df, pred_anomaly='pred_anomaly_iforest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Submission - make predictions on test set with Isolation Forest (Score: 0.72552; Private score: 0.73704)\n",
    "iforest_df_test = iforest_model(test)\n",
    "iforest_df_test_submit = iforest_df_test[['row_id', 'anomaly_score']].rename(columns={'row_id': 'row_id', 'anomaly_score': 'anomaly'}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(train_with_anomalies, building_id=108, pred_anomaly='pred_anomaly_iqr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
